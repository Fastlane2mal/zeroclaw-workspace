# Services

## Service Overview
All services start automatically on boot. Managed via user-level systemd.

---

## Ollama
- **Purpose**: Runs local AI models on the CPU
- **Port**: 11434
- **Manages**: systemctl --user (ollama.service)
- **Models loaded**: smollm2, qwen2.5-1.5b
- **Data stored at**: `/mnt/hdd/llms/` (model files)
- **Key config**: Bound to `0.0.0.0` so containers can reach it

**Start/stop**:
```bash
systemctl --user start ollama
systemctl --user stop ollama
systemctl --user status ollama
```

---

## LiteLLM
- **Purpose**: Unified API gateway — one endpoint for all models (local and cloud)
- **Port**: 4000
- **Manages**: Podman container via systemd quadlet
- **Config file**: `~/.litellm/config.yaml`
- **API keys**: `~/.config/litellm.env`

**Models it routes to**:
- `smollm2` → Ollama local
- `qwen2.5-1.5b` → Ollama local
- `llama-3.3-70b` → Groq cloud (free, rate limited)
- `llama-3.1-8b` → Groq cloud (free, rate limited)
- `mixtral-8x7b` → Groq cloud (free, rate limited)
- `claude-haiku-4` → Anthropic cloud (paid)
- `claude-sonnet-4` → Anthropic cloud (paid)

**Start/stop**:
```bash
systemctl --user start litellm
systemctl --user stop litellm
systemctl --user status litellm
```

**Test it's working**:
```bash
curl -s http://localhost:4000/health -H "Authorization: Bearer ${LITELLM_MASTER_KEY}"
```

---

## AnythingLLM
- **Purpose**: Web-based chat UI with document upload and RAG (Q&A over your own documents)
- **Port**: 3001
- **Manages**: Podman container
- **App data stored at**: `/mnt/hdd/projects/anythingllm/` (workspaces, settings, chat history)
- **Document source**: `/mnt/hdd/share/` (entire Samba share, mounted into container)
- **Access**: Requires SSH tunnel from Windows PC

### Volume Mounts
| Host Path | Container Path | Purpose |
|---|---|---|
| `/mnt/hdd/projects/anythingllm` | `/app/server/storage` | AnythingLLM app data |
| `/mnt/hdd/share` | `/app/server/storage/hotdir` | Document source (full Samba share) |

### Why the Full Share Is Mounted
The entire Samba share (`F:\` on Windows) is mounted so that any project folder can be used as a document source without reconfiguring the container. New projects are added by creating a new workspace in AnythingLLM and selecting the relevant folder — no server changes needed.

### Path Mapping (Windows ↔ Server)
```
Windows (Logseq)                        Server (AnythingLLM sees)
F:\Projects\SilverblueAI\pages    →    /mnt/hdd/share/Projects/SilverblueAI/pages
F:\Projects\NewProject\pages      →    /mnt/hdd/share/Projects/NewProject/pages
F:\Projects\AnyOther\docs         →    /mnt/hdd/share/Projects/AnyOther/docs
```

### Full Container Launch Command
```bash
podman run -d \
  --name anythingllm \
  --network host \
  --restart=always \
  -v /mnt/hdd/projects/anythingllm:/app/server/storage:Z \
  -v /mnt/hdd/share:/app/server/storage/hotdir:Z \
  ghcr.io/mintplex-labs/anythingllm:master
```

### Relaunch Procedure (if container config needs changing)
```bash
# Stop and remove container (data on HDD is safe)
podman stop anythingllm
podman rm anythingllm

# Relaunch with updated command above
podman run -d \
  --name anythingllm \
  --network host \
  --restart=always \
  -v /mnt/hdd/projects/anythingllm:/app/server/storage:Z \
  -v /mnt/hdd/share:/app/server/storage/hotdir:Z \
  ghcr.io/mintplex-labs/anythingllm:master

# Verify
podman ps | grep anythingllm
podman logs anythingllm --tail 20
```

### Access from Windows
```powershell
# Open PowerShell and run:
ssh -L 3001:localhost:3001 silverblue-ai
# Then open browser to: http://localhost:3001
```

### Start/stop
```bash
podman start anythingllm
podman stop anythingllm
podman restart anythingllm
podman logs anythingllm --tail 50
podman logs anythingllm -f
```

### Adding a New Project
1. Save your Logseq graph to a folder on F:\ (e.g. `F:\Projects\NewProject\`)
2. In AnythingLLM, create a new workspace
3. Click the document icon → browse the hotdir library to your project's pages folder
4. Select files and assign them to the workspace
5. No server changes needed — ever

### SELinux Note
If AnythingLLM cannot read files written via Samba, run:
```bash
sudo restorecon -R /mnt/hdd/share/
```

---

## Caddy (HTTPS Proxy)
- **Purpose**: Provides HTTPS access to AnythingLLM (optional, not always running)
- **Port**: 8443
- **Status**: Available but not set to auto-start
- **Start manually**: `podman start caddy-https`

---

## Tailscale (VPN)
- **Purpose**: Secure remote access without exposing ports to the internet
- **Tailscale IP**: 100.110.112.76
- **Manages**: System-level service (tailscaled)
- **Install on other devices**: https://tailscale.com

**Check status**:
```bash
tailscale status
```

---

## Samba (File Sharing)
- **Purpose**: Access the HDD from Windows as a mapped network drive
- **Manages**: System-level service (smb + nmb)
- **Share path on server**: `/mnt/hdd/share/`
- **Windows mapped drive**: F:\
- **Access from Windows**: Map network drive to `\\silverblue-ai\share` or `\\100.110.112.76\share`

**Note**: This is the same folder AnythingLLM reads documents from. Files saved to F:\ on Windows are immediately visible to AnythingLLM on the server.

---

## SSH
- **Purpose**: Remote terminal access to the server
- **Port**: 22
- **Auth**: Key-based only (no password)
- **Access**: `ssh silverblue-ai` (if configured in ~/.ssh/config on Windows)

---

## Quick Health Check (Run This Each Session)
```bash
# Are core services up?
systemctl --user status ollama litellm

# Is the API responding?
curl -s http://localhost:4000/health -H "Authorization: Bearer ${LITELLM_MASTER_KEY}"

# What models are available?
curl -s http://localhost:4000/v1/models -H "Authorization: Bearer ${LITELLM_MASTER_KEY}" | jq -r '.data[].id'

# How is RAM and disk?
free -h | grep Mem
df -h /mnt/hdd

# Is AnythingLLM running?
podman ps | grep anythingllm
```

## Related
- [[Architecture]] — how services connect
- [[Commands Reference]] — all useful commands
- [[Known Issues]] — things that have gone wrong

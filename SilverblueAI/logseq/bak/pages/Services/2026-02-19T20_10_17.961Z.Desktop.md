# Services

## Service Overview
All services start automatically on boot. Managed via user-level systemd.

---

## Ollama
- **Purpose**: Runs local AI models on the CPU
- **Port**: 11434
- **Manages**: systemctl --user (ollama.service)
- **Models loaded**: smollm2, qwen2.5-1.5b
- **Data stored at**: `/mnt/hdd/llms/` (model files)
- **Key config**: Bound to `0.0.0.0` so containers can reach it

**Start/stop**:
```bash
systemctl --user start ollama
systemctl --user stop ollama
systemctl --user status ollama
```

---

## LiteLLM
- **Purpose**: Unified API gateway — one endpoint for all models (local and cloud)
- **Port**: 4000
- **Manages**: Podman container via systemd quadlet
- **Config file**: `~/.litellm/config.yaml`
- **API keys**: `~/.config/litellm.env`

**Models it routes to**:
- `smollm2` → Ollama local
- `qwen2.5-1.5b` → Ollama local
- `llama-3.3-70b` → Groq cloud (free, rate limited)
- `llama-3.1-8b` → Groq cloud (free, rate limited)
- `mixtral-8x7b` → Groq cloud (free, rate limited)
- `claude-haiku-4` → Anthropic cloud (paid)
- `claude-sonnet-4` → Anthropic cloud (paid)

**Start/stop**:
```bash
systemctl --user start litellm
systemctl --user stop litellm
systemctl --user status litellm
```

**Test it's working**:
```bash
curl -s http://localhost:4000/health -H "Authorization: Bearer ${LITELLM_MASTER_KEY}"
```

---

## AnythingLLM
- **Purpose**: Web-based chat UI with document upload and RAG (Q&A over your documents)
- **Port**: 3001
- **Manages**: Podman container via systemd quadlet
- **Data stored at**: `/mnt/hdd/projects/anythingllm/`
- **Access**: Requires SSH tunnel from Windows PC

**Access from Windows**:
```powershell
# Open PowerShell and run:
ssh -L 3001:localhost:3001 silverblue-ai
# Then open browser to: http://localhost:3001
```

**Start/stop**:
```bash
systemctl --user start anythingllm  # or via podman
podman start anythingllm
podman stop anythingllm
podman logs anythingllm --tail 50
```

---

## Caddy (HTTPS Proxy)
- **Purpose**: Provides HTTPS access to AnythingLLM (optional, not always running)
- **Port**: 8443
- **Status**: Available but not set to auto-start
- **Start manually**: `podman start caddy-https`

---

## Tailscale (VPN)
- **Purpose**: Secure remote access without exposing ports to the internet
- **Tailscale IP**: 100.110.112.76
- **Manages**: System-level service (tailscaled)
- **Install on other devices**: https://tailscale.com

**Check status**:
```bash
tailscale status
```

---

## Samba (File Sharing)
- **Purpose**: Access the HDD from Windows via network drive
- **Manages**: System-level service (smb + nmb)
- **Share path**: `/mnt/hdd`
- **Access from Windows**: Map network drive to `\\silverblue-ai\hdd` or `\\100.110.112.76\hdd`

---

## SSH
- **Purpose**: Remote terminal access to the server
- **Port**: 22
- **Auth**: Key-based only (no password)
- **Access**: `ssh silverblue-ai` (if configured in ~/.ssh/config on Windows)

---

## Quick Health Check (Run This Each Session)
```bash
# Are core services up?
systemctl --user status ollama litellm

# Is the API responding?
curl -s http://localhost:4000/health -H "Authorization: Bearer ${LITELLM_MASTER_KEY}"

# What models are available?
curl -s http://localhost:4000/v1/models -H "Authorization: Bearer ${LITELLM_MASTER_KEY}" | jq -r '.data[].id'

# How is RAM and disk?
free -h | grep Mem
df -h /mnt/hdd

# Is AnythingLLM running?
podman ps | grep anythingllm
```

## Related
- [[Architecture]] — how services connect
- [[Commands Reference]] — all useful commands
- [[Known Issues]] — things that have gone wrong

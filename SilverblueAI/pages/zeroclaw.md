# ZeroClaw Deployment Guide

**Purpose**: Deploy ZeroClaw lightweight AI agent on the Silverblue AI Platform  
**Prerequisites**: Stable baseline (Ollama + LiteLLM working), AnythingLLM removed  
**Estimated Time**: 75-90 minutes (includes reboot for build deps)  
**Platform**: Fedora Silverblue (SSH from Windows)  
**Version**: 1.9 (February 2026)

---

## Overview

ZeroClaw is a Rust-based AI agent framework (~3.4MB binary, <5MB RAM) that provides a 24/7 AI assistant accessible via Telegram, Discord, CLI, and other channels. It connects directly to your local Ollama instance and routes through LiteLLM for cloud model access.

### Deployment Approach: Host Binary (Option B)

ZeroClaw is deployed as a **user-level host binary** at `~/.local/bin/zeroclaw`. This is appropriate for Silverblue because:

- It is a single static Rust binary with no runtime dependencies
- It stays user-level â€” no root, no system directories
- `~/.local/bin` is already in PATH on Fedora by default
- The systemd user service references the binary directly
- Config lives at `~/.zeroclaw/config.toml` (generated by onboarding wizard)

The rest of the AI stack (LiteLLM, Caddy) runs in Podman containers. ZeroClaw is small enough that containerising it adds complexity without benefit. **Option A (Podman container)** remains available and is documented at the end of this guide if you want to migrate later.

### Why ZeroClaw on This Hardware
|---|---|---|
| **Binary size** | ~400MB runtime | ~3.4MB |
| **RAM usage** | 200-400MB | <5MB |
| **Token usage/request** | 13,000+ (breaks Groq free tier) | ~2,000-4,000 |
| **Security model** | CVEs, compromised instances | Gateway pairing, sandbox, allowlists |
| **Ollama integration** | Via LiteLLM only | Native or via LiteLLM |
| **Channels** | Telegram only | Telegram, Discord, Slack, iMessage, Matrix, Webhook |

### Architecture After This Deployment

```
Telegram / Windows PC (via Tailscale)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ZeroClaw      â”‚  â† ~10MB RAM, systemd user service (zeroclaw daemon)
â”‚   (Rust agent)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â†’ LiteLLM (localhost:4000)      [default â€” claude-haiku-4, ~1-2s]
         â”‚    â””â”€ Anthropic Claude Haiku     [reliable tool-calling, paid]
         â”‚
         â”œâ”€â”€â†’ OpenRouter (cloud, native)    [fallback â€” gemini-2.0-flash-001]
         â”‚
         â””â”€â”€â†’ Ollama (localhost:11434)      [local â€” mistral, qwen2.5:1.5b]
```

**Provider decision**: `custom:http://localhost:4000/v1` + `claude-haiku-4` is the recommended default â€” reliable tool-calling, fast responses, routes through LiteLLM which handles Anthropic API translation. OpenRouter/Gemini is a good fallback if Anthropic credits run out. Groq native provider has a v0.1.1 bug (wrong API endpoint). Use `custom:` not `anthropic-custom:` â€” the latter sends the wrong format to LiteLLM.

---

## Pre-Flight: Remove AnythingLLM

If AnythingLLM is currently running on the server, remove it before proceeding to free RAM and port 3001.

**Note**: Your Windows AnythingLLM can still reach your server's LiteLLM via Tailscale at `100.110.112.76:4000` â€” nothing is lost.

### Step A.1 â€” Stop and Remove AnythingLLM Container

```bash
# SSH into server
ssh silverblue-ai

# Stop the container
# Note: AnythingLLM ignores SIGTERM â€” Podman will warn and escalate to SIGKILL after 10s
# The warning is normal and harmless â€” data lives in the bind-mounted HDD volume
podman stop anythingllm

# Force-remove the container (handles any residual running state from SIGKILL)
podman rm --force anythingllm

# Remove the image â€” actual registry tag is docker.io not ghcr.io
podman rmi docker.io/mintplexlabs/anythingllm:latest

# Clean up any orphaned image layers
podman image prune -f
```

### Step A.2 â€” Verify Data is Preserved

```bash
# Confirm data volume is intact (DO NOT delete this)
ls -lh /mnt/hdd/projects/anythingllm-storage/
```

Data is preserved at `/mnt/hdd/projects/anythingllm-storage`. If you ever reinstall AnythingLLM on the server, it will resume exactly where it left off.

### Step A.3 â€” Verify Resources Freed

```bash
# Check RAM freed (~300-800MB should now be available)
free -h

# Confirm port 3001 is no longer in use
sudo ss -tlnp | grep 3001
# Should return nothing

# Confirm container is gone
podman ps --all | grep anythingllm
# Should return nothing
```

**CHECKPOINT A**: AnythingLLM removed, RAM freed, data preserved on HDD.

---

## Phase 1: Update LiteLLM Model Configuration

Replace smollm2 with qwen2.5:3b as the primary local model â€” better instruction-following for agentic tool use.

### Step 1.1 â€” Pull the New Model

```bash
# Pull qwen2.5:3b (~2GB download to /mnt/hdd/llms)
ollama pull qwen2.5:3b

# Monitor progress
ollama list
```

### Step 1.2 â€” Test the New Model

```bash
time curl -s http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:3b",
  "prompt": "Reply in exactly 5 words: what is 2+2?",
  "stream": false
}' | jq -r '.response'

# Expected: 15-25 second response on i5-8250U
```

### Step 1.3 â€” Optionally Remove smollm2

```bash
# Remove smollm2 to free ~1GB HDD (optional)
ollama rm smollm2

# Verify remaining models
ollama list
# Should show: qwen2.5:3b, qwen2.5:1.5b
```

### Step 1.4 â€” Update LiteLLM Config

```bash
cat > ~/.litellm/config.yaml << 'EOF'
litellm_settings:
  drop_params: true

model_list:
  # Primary agent model â€” better instruction following for tool use
  - model_name: qwen2.5:3b
    litellm_params:
      model: ollama/qwen2.5:3b
      api_base: http://localhost:11434

  # Lightweight local fallback â€” fast, simple queries
  - model_name: qwen2.5-1.5b
    litellm_params:
      model: ollama/qwen2.5:1.5b
      api_base: http://localhost:11434

  # Fast cloud reasoning via Groq (FREE, rate limited)
  - model_name: llama-3.3-70b
    litellm_params:
      model: groq/llama-3.3-70b-versatile
      api_key: os.environ/GROQ_API_KEY

  - model_name: llama-3.1-8b
    litellm_params:
      model: groq/llama-3.1-8b-instant
      api_key: os.environ/GROQ_API_KEY

  # Premium cloud (PAID â€” select manually)
  - model_name: claude-haiku-4
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: claude-sonnet-4
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: os.environ/ANTHROPIC_API_KEY

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
EOF
```

### Step 1.5 â€” Restart LiteLLM and Verify

```bash
systemctl --user restart litellm
sleep 20

# Verify model list
curl -s http://localhost:4000/v1/models \
  -H "Authorization: Bearer $(grep LITELLM_MASTER_KEY ~/.silverblue-ai-config | cut -d'"' -f2)" \
  | jq -r '.data[].id'

# Should list: qwen2.5:3b, qwen2.5-1.5b, llama-3.3-70b, llama-3.1-8b, claude-haiku-4, claude-sonnet-4

# Test inference through LiteLLM
curl -s http://localhost:4000/v1/chat/completions \
  -H "Authorization: Bearer $(grep LITELLM_MASTER_KEY ~/.silverblue-ai-config | cut -d'"' -f2)" \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5:3b", "messages": [{"role":"user","content":"Say hello in 3 words"}]}' \
  | jq -r '.choices[0].message.content'
```

**CHECKPOINT 1**: New model config active, qwen2.5:3b responding via LiteLLM.

---

## Phase 2: Install Build Dependencies, Rust, and ZeroClaw

ZeroClaw is not on crates.io â€” it must be built from source. Fedora Silverblue's immutable OS requires installing all build tools via `rpm-ostree` (which needs a reboot) before ZeroClaw can be compiled.

### Step 2.1 â€” Install Build Dependencies and Rust via rpm-ostree

This is the correct Silverblue approach â€” layering packages onto the OS image rather than using ad-hoc installers. Everything goes in one reboot.

```bash
# Install gcc and openssl-devel first (--allow-inactive skips already-satisfied packages)
rpm-ostree install gcc openssl-devel --allow-inactive

# Then install Rust and Cargo as layered OS packages
rpm-ostree install rust cargo --allow-inactive

# Reboot to activate all layered packages
sudo systemctl reboot
```

Wait ~90 seconds, then SSH back in:

```bash
ssh silverblue-ai

# Verify everything is available
rustc --version
cargo --version
gcc --version
# All three should print version info
```

**Why this approach over rustup**: Layering rust/cargo via rpm-ostree keeps them in the OS image, survives OS updates cleanly, and is consistent with how everything else on this system is managed. No `~/.cargo/env` sourcing required, no separate toolchain manager.

### Step 2.2 â€” Clone ZeroClaw from GitHub

ZeroClaw is hosted at `zeroclaw-labs/zeroclaw` â€” not the RightNow-AI fork referenced in earlier research.

```bash
# Clone the canonical repository
git clone https://github.com/zeroclaw-labs/zeroclaw.git
cd zeroclaw
```

### Step 2.3 â€” Build and Install to ~/.local/bin

```bash
# Build release binary (3-8 minutes on i5-8250U â€” 6GB+ free RAM is fine)
cargo build --release --locked

# Create ~/.local/bin if it doesn't exist
mkdir -p ~/.local/bin

# Copy binary â€” ~/.local/bin is already in PATH on Fedora, no cargo env needed
cp target/release/zeroclaw ~/.local/bin/zeroclaw

# Verify
zeroclaw --version
which zeroclaw
# Should show: ~/.local/bin/zeroclaw
```

### Step 2.4 â€” Optionally Remove Build Tools (Recommended)

The layered build tools (gcc, rust, cargo, openssl-devel) are no longer needed once the binary is compiled. Removing them keeps the OS layer lean. This requires one more reboot.

```bash
rpm-ostree uninstall gcc rust cargo openssl-devel --allow-inactive

sudo systemctl reboot
```

After reboot, verify the binary is still accessible (it lives in your home directory â€” uninstalling the build tools doesn't touch it):

```bash
ssh silverblue-ai
zeroclaw --version
# Should still work â€” binary is at ~/.local/bin/zeroclaw
```

**Skip this step** if you anticipate rebuilding ZeroClaw again soon (e.g. to pick up upstream updates). You can always uninstall the build tools later.

**CHECKPOINT 2**: ZeroClaw binary at `~/.local/bin/zeroclaw`, accessible without any PATH configuration.

---

## Phase 3: Configure ZeroClaw (Onboarding Wizard)

ZeroClaw uses an interactive onboarding wizard that generates `~/.zeroclaw/config.toml`. Run it once after the binary is installed.

### Step 3.1 â€” Create Data Directory

```bash
mkdir -p /mnt/hdd/projects/zeroclaw
```

### Step 3.2 â€” Run the Onboarding Wizard

```bash
zeroclaw onboard --interactive
```

**Answer each step as follows:**

**[1/9] Workspace Setup** â†’ Accept default `~/.zeroclaw/workspace`

**[2/9] AI Provider & API Key**
- Category â†’ `ðŸ  Local / private`
- Provider â†’ `Ollama â€” local models`
- Remote endpoint? â†’ `N`

**[3/9] Default Model** â†’ `qwen2.5:1.5b` (faster on this hardware; switch to Groq after setup)

**[4/9] Tunnel** â†’ `Tailscale â€” private tailnet or public Funnel` â†’ select **private tailnet**

**[5/9] Encrypted Secret Storage** â†’ `Y`

**[6/9] Hardware Access** â†’ `â˜ï¸ Software Only`

**[7/9] Channel** â†’ `Telegram` â€” have your bot token from @BotFather ready. Get your Telegram user ID from `@userinfobot`.

**[8/9] & [9/9]** â€” Follow remaining prompts to complete.

### Step 3.3 â€” Fix Config Permissions

```bash
chmod 600 ~/.zeroclaw/config.toml
```

### Step 3.4 â€” Set Default Provider (Claude Haiku via LiteLLM)

**Provider compatibility notes for ZeroClaw v0.1.1:**

- **`custom:` (LiteLLM) + claude-haiku-4** â€” âœ… Working. Reliable tool-calling, fast, routes through your existing LiteLLM stack. **Recommended default.**
- **OpenRouter + gemini-2.0-flash-001** â€” âœ… Working. Good fallback if Anthropic credits run out.
- **`anthropic-custom:` (LiteLLM)** â€” âŒ Do not use. Sends requests in wrong format, returns 404.
- **Groq (native)** â€” âŒ Broken. Sends requests to `/openai/responses` which doesn't exist on Groq.
- **Ollama** â€” âœ… Working but slow (15-25s). Use for offline/free use only.

Update config.toml:

```bash
nano ~/.zeroclaw/config.toml
```

```toml
default_provider = "custom:http://localhost:4000/v1"
default_model = "claude-haiku-4"
api_key = "your-litellm-master-key"
```

Your LiteLLM master key is in `~/.silverblue-ai-config` as `LITELLM_MASTER_KEY`.

**Critical LiteLLM config requirement**: Anthropic models in `~/.litellm/config.yaml` must use the `anthropic/` prefix in the model string, otherwise LiteLLM throws "LLM Provider NOT provided" and skips the deployment silently:

```yaml
  - model_name: claude-haiku-4
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001    # anthropic/ prefix required
      api_key: os.environ/ANTHROPIC_API_KEY
```

### Step 3.5 â€” Enable Web Research

To allow ZeroClaw to research topics by searching the web and fetching page content, two changes are needed in `config.toml`:

```bash
nano ~/.zeroclaw/config.toml
```

**1. Enable web search and HTTP fetch:**

```toml
[web_search]
enabled = true
provider = "duckduckgo"
max_results = 10
timeout_secs = 15

[http_request]
enabled = true
allowed_domains = []
max_response_size = 1048576
timeout_secs = 30
```

**2. Add them to `auto_approve` in the `[autonomy]` section** â€” this is the critical step. Without it ZeroClaw won't offer these tools to the model:

```toml
auto_approve = [
    "file_read",
    "memory_recall",
    "web_search",
    "http_fetch",
]
```

**Note**: `workspace_only` does not need to be changed â€” it was already `false` by default. The `auto_approve` list is what controls whether tools are surfaced to the model.

Test:
```bash
zeroclaw agent -m "Search the web for the latest Rust news and summarise what you find"
# ZeroClaw should search DuckDuckGo, fetch page content, and synthesise a response
# Not fall back to general knowledge
```

**CHECKPOINT 3**: ZeroClaw configured, OpenRouter/Gemini as default provider, web research enabled.

---

## Phase 4: Set Up Environment Variables

The onboarding wizard handles most configuration via `~/.zeroclaw/config.toml`. The only environment variable you may need to add manually is the Telegram bot token if you didn't enter it during the wizard, or if you want it stored in your master config rather than the ZeroClaw config.

### Step 4.1 â€” Verify Master Config Has What's Needed

```bash
# Check LITELLM_MASTER_KEY is present (needed for LiteLLM routing)
grep LITELLM_MASTER_KEY ~/.silverblue-ai-config

# If using Telegram and token not already in ZeroClaw config, add it:
nano ~/.silverblue-ai-config
# Add: export TELEGRAM_BOT_TOKEN="your-token-here"
```

### Step 4.2 â€” Reload Config

```bash
source ~/.silverblue-ai-config

# Verify key is loaded
echo $LITELLM_MASTER_KEY
# Should print your key (not empty)
```

**CHECKPOINT 4**: Environment variables confirmed.

---

## Phase 5: Test ZeroClaw Manually

Verify ZeroClaw works correctly before relying on the service.

### Step 5.1 â€” CLI Test via OpenRouter (Primary)

```bash
zeroclaw agent -m "What is 2 + 2? Reply in exactly 5 words."
# Expected: 1-3 second response via Gemini Flash
```

### Step 5.2 â€” Web Research Test

```bash
zeroclaw agent -m "Search the web for the latest Rust news and summarise what you find"
# Expected: ZeroClaw searches DuckDuckGo, fetches pages, synthesises a response
```

### Step 5.3 â€” File Write Test (Known Issue)

File writing via the `file_write` tool is currently unreliable in ZeroClaw v0.1.0. The model correctly generates tool calls but ZeroClaw does not execute them â€” the tool call appears as a `tool_code` block in the response rather than being dispatched. This affects all providers tested (LiteLLM custom:, OpenRouter/Gemini).

```bash
zeroclaw agent -m "Write the text 'test' to /mnt/hdd/share/claw-test.txt"
# Current behaviour: model describes the file_write call but no file is created
# Check: ls /mnt/hdd/share/claw-test.txt
```

Check [github.com/zeroclaw-labs/zeroclaw/issues](https://github.com/zeroclaw-labs/zeroclaw/issues) for upstream fix status before attempting workarounds.

### Step 5.4 â€” Run Daemon Mode Test

```bash
zeroclaw daemon
# Send a Telegram message â€” should respond at Gemini speed
# Ctrl+C to stop
```

### Step 5.5 â€” Diagnostics

```bash
zeroclaw doctor
zeroclaw status
```

**CHECKPOINT 5**: ZeroClaw responding via OpenRouter, web research working, Telegram confirmed.

---

## Phase 6: Install systemd User Service

ZeroClaw's `service install` command generates and registers the systemd user service automatically. The service runs `zeroclaw daemon` â€” which includes the gateway, Telegram channel, scheduler, and heartbeat all in one process.

### Step 6.1 â€” Install the Service

```bash
zeroclaw service install

systemctl --user daemon-reload
systemctl --user enable zeroclaw
systemctl --user start zeroclaw

sleep 5
systemctl --user status zeroclaw
```

Expected:
```
â— zeroclaw.service - ZeroClaw AI Agent
     Loaded: loaded (...; enabled)
     Active: active (running) since ...
```

### Step 6.2 â€” Verify the Generated Service File

```bash
cat ~/.config/systemd/user/zeroclaw.service
```

Check that `ExecStart` points to `~/.local/bin/zeroclaw daemon` (not `serve`). If it's wrong or env vars are missing, edit manually:

```bash
nano ~/.config/systemd/user/zeroclaw.service

# ExecStart should be:
# ExecStart=%h/.local/bin/zeroclaw daemon
#
# Add under [Service] if env vars are missing:
# EnvironmentFile=%h/.silverblue-ai-config

systemctl --user daemon-reload
systemctl --user restart zeroclaw
```

### Step 6.3 â€” Verify Logs and Telegram

```bash
journalctl --user -u zeroclaw -f
# Look for: gateway started, Telegram channel connected, scheduler ready

# Send a Telegram message to your bot to confirm end-to-end
```

### Step 6.4 â€” Run Doctor

```bash
zeroclaw doctor
# All items should now show âœ… including daemon state
```

**CHECKPOINT 6**: ZeroClaw daemon running as systemd service, auto-starts on boot, Telegram responding.

---

## Phase 7: Firewall Verification

ZeroClaw's gateway binds to `127.0.0.1:3000` (localhost only â€” confirmed in config.toml). No firewall changes needed. Telegram is outbound-only.

```bash
# Confirm localhost-only binding (port 3000, not 8080)
sudo ss -tlnp | grep 3000
# Should show: 127.0.0.1:3000 (NOT 0.0.0.0:3000)

# Confirm firewall unchanged
sudo firewall-cmd --list-all | grep port
# Should still show only SSH and Samba ports
```

**Remote gateway access** (if needed) via SSH tunnel through Tailscale:
```bash
# On Windows:
ssh -L 3000:localhost:3000 silverblue-ai.YOUR-TAILNET.ts.net
# Then access http://localhost:3000 on Windows
```

**CHECKPOINT 7**: Security posture unchanged, gateway localhost-bound.

---

## Phase 8: Reboot Verification

```bash
sudo systemctl reboot
```

After ~90 seconds:

```bash
ssh silverblue-ai

# Verify all three AI services auto-started
systemctl --user status ollama litellm zeroclaw

# All three should show: active (running)

# End-to-end test
zeroclaw agent -m "Hello, are you working after reboot?"

# Resource check (expect ~4-5GB used)
free -h
```

**CHECKPOINT 8**: Full stack operational after reboot.

---

## Quick Reference

### Service Management

```bash
systemctl --user start|stop|restart zeroclaw
journalctl --user -u zeroclaw -f
journalctl --user -u zeroclaw -n 100
systemctl --user status ollama litellm zeroclaw

# Diagnostics
zeroclaw doctor
zeroclaw status
```

### Model / Provider Switching

```bash
# Default (Claude Haiku via LiteLLM, fast, reliable tool-calling)
zeroclaw agent -m "message"

# Explicitly use Claude Haiku via LiteLLM
zeroclaw agent \
  --provider "custom:http://localhost:4000/v1" \
  --model claude-haiku-4 -m "message"

# OpenRouter/Gemini (good fallback if Anthropic credits exhausted)
zeroclaw agent --provider openrouter --model google/gemini-2.0-flash-001 -m "message"

# Local Ollama (~10-25s, no rate limits, no cost)
zeroclaw agent --provider ollama --model qwen2.5:1.5b -m "message"
zeroclaw agent --provider ollama --model mistral -m "message"
```

**Provider status summary (v0.1.1):**
- `custom:` (LiteLLM) + `claude-haiku-4` â†’ âœ… Working, fast, reliable tool-calling â€” **recommended default**
- `openrouter` + Gemini Flash â†’ âœ… Working, fast, good tool-calling â€” good fallback
- `ollama` â†’ âœ… Working, slow, no rate limits or cost
- `anthropic-custom:` (LiteLLM) â†’ âŒ Do not use â€” sends wrong format, returns 404
- `groq` (native) â†’ âŒ Broken â€” wrong API endpoint in v0.1.1

**Important LiteLLM config notes**:
- Anthropic model names require `anthropic/` prefix: `anthropic/claude-haiku-4-5-20251001`
- Without the prefix LiteLLM throws "LLM Provider NOT provided" and skips the deployment
- Use `custom:` not `anthropic-custom:` in ZeroClaw â€” LiteLLM handles the Anthropic translation internally

### Supported Providers (32 total)

ZeroClaw has native support for: `groq`, `ollama`, `anthropic`, `openai`, `openrouter`, `gemini`, `mistral`, `deepseek`, `together`, `fireworks`, `perplexity`, and more. Full list: `zeroclaw providers`

For any OpenAI-compatible endpoint: `custom:<URL>`
For any Anthropic-compatible endpoint: `anthropic-custom:<URL>`

### Key Paths

| Item | Location |
|------|----------|
| Binary | `~/.local/bin/zeroclaw` (built from source) |
| Config | `~/.zeroclaw/config.toml` (chmod 600) |
| Workspace | `~/.zeroclaw/workspace` |
| Memory DB | `~/.zeroclaw/workspace/memory.db` (default) |
| Service file | `~/.config/systemd/user/zeroclaw.service` |
| Logs | `journalctl --user -u zeroclaw` |
| Source | `~/zeroclaw/` (keep for rebuilds/updates) |

---

## Troubleshooting

**Config file world-readable warning** â€” Run `chmod 600 ~/.zeroclaw/config.toml`. ZeroClaw warns on every startup until resolved.

**`initialized=false` in logs** â€” Wizard completion flag, does not prevent ZeroClaw from working. Ignore it.

**"Unknown provider: gateway" error** â€” `gateway` is not a valid provider. Use `openrouter`, `ollama`, or `custom:<URL>`.

**"Custom API key not set" error** â€” Add `api_key = "your-key"` as a top-level entry in `config.toml`. Environment variables are not automatically picked up for custom providers.

**Groq native provider fails with "Unknown request URL: POST /openai/responses"** â€” This is a ZeroClaw v0.1.0 bug. Do not use `default_provider = "groq"`. Use OpenRouter instead. Check GitHub issues for fix status.

**OpenRouter invalid model ID error** â€” Model IDs must be exact. `google/gemini-2.0-flash` is invalid â€” use `google/gemini-2.0-flash-001`. Check [openrouter.ai/models](https://openrouter.ai/models) for current IDs.

**Using `anthropic-custom:` provider returns 404** â€” Use `custom:http://localhost:4000/v1` instead. LiteLLM exposes an OpenAI-compatible endpoint â€” `anthropic-custom:` sends the wrong request format. ZeroClaw should always use `custom:` to talk to LiteLLM regardless of which underlying model LiteLLM routes to.

**Claude shows as "no healthy deployments" in LiteLLM** â€” The model string in `config.yaml` is missing the `anthropic/` provider prefix. Change `model: claude-haiku-4-5-20251001` to `model: anthropic/claude-haiku-4-5-20251001`. Without the prefix LiteLLM cannot determine the provider and silently skips the deployment on startup.

**Duplicate key TOML parse error** â€” Each top-level key must appear exactly once. Common cause: editing config manually and creating duplicate `[autonomy]` or `default_provider` entries.

**TOML parse error: missing field** â€” Required fields were accidentally deleted during editing. Common victims: `workspace_only`, `max_actions_per_hour`. Restore from the known-good config in this guide.

**"Address already in use" when starting daemon** â€” ZeroClaw service is already running. Don't run `zeroclaw daemon` manually â€” use `systemctl --user restart zeroclaw` instead.

**Telegram 409 conflict** â€” Two ZeroClaw instances running simultaneously. Run `pkill -f zeroclaw`, then `systemctl --user start zeroclaw`.

**ZeroClaw fails to start**
```bash
journalctl --user -u zeroclaw -n 50
# TOML parse error â†’ fix reported line in config.toml
# "Connection refused" â†’ Ollama not running: systemctl --user start ollama
# "Permission denied" â†’ chown -R $USER:$USER ~/.zeroclaw
```

**Web search returns results but model uses general knowledge** â€” Add `web_search` and `http_fetch` to `auto_approve` in `[autonomy]`. `workspace_only` does not need changing.

**Slow responses** â€” Switch from Ollama to OpenRouter: `default_provider = "openrouter"`, `default_model = "google/gemini-2.0-flash-001"`.

**Telegram not responding** â€” Check `journalctl --user -u zeroclaw -f`. Verify `bot_token` in `[channels_config.telegram]` is correct. If you lost the conversation in Telegram, search for your bot by username and send `/start`.

---

## Known Issues

### Tool Execution â€” Fixed in v0.1.1

**Status**: âœ… Resolved â€” update to v0.1.1  
**Was affected**: ZeroClaw v0.1.0 â€” all providers  
**Symptom**: Model generated `tool_code` blocks that ZeroClaw returned as text without executing. ~600ms response time confirmed no dispatch was occurring.  
**Fix**: `git pull && cargo build --release --locked && cp target/release/zeroclaw ~/.local/bin/zeroclaw`  
**Verified working**: File write to workspace confirmed in v0.1.1

### workspace Key in config.toml Ignored

**Status**: Unresolved â€” v0.1.1 limitation  
**Symptom**: Setting `workspace = "/path"` in config.toml has no effect. ZeroClaw always uses `~/.zeroclaw/workspace` regardless.  
**Workaround**: Add the workspace as a Samba share instead â€” see "Exposing Workspace via Samba" below.

### Groq Native Provider Broken

**Status**: Unresolved â€” check GitHub for fix status  
**Symptom**: All calls fail with `Unknown request URL: POST /openai/responses`  
**Workaround**: Use OpenRouter instead. Do not use `default_provider = "groq"`.

### Free Tier Rate Limits on OpenRouter

**Status**: Expected behaviour  
**Symptom**: `:free` model variants return 429 errors during peak hours  
**Workaround**: Use non-free model variants (small cost) or retry during off-peak hours.

---

## Exposing Workspace via Samba

ZeroClaw enforces its workspace boundary strictly â€” it will not write to paths outside `~/.zeroclaw/workspace` even with `workspace_only = false`. The `workspace` config key is currently ignored.

The simplest way to access ZeroClaw's files from Windows is to add the workspace as a dedicated Samba share:

```bash
sudo nano /etc/samba/smb.conf
```

Add at the end:
```ini
[zeroclaw]
   path = /var/home/mal/.zeroclaw/workspace
   browseable = yes
   read only = no
   valid users = mal
   create mask = 0644
   directory mask = 0755
```

Restart Samba:
```bash
sudo systemctl restart smb
```

Access from Windows:
```
\\silverblue-ai\zeroclaw
```

ZeroClaw writes files to its workspace as normal, and they appear immediately in the Windows share. No path changes or symlinks required.

---



If you later want to containerise ZeroClaw to align it fully with the rest of the stack (LiteLLM, Caddy), the migration path is:

1. Create a `Containerfile` in the zeroclaw source directory:
```dockerfile
FROM rust:bookworm AS builder
WORKDIR /app
COPY . .
RUN cargo build --release --locked

FROM debian:bookworm-slim
RUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*
COPY --from=builder /app/target/release/zeroclaw /usr/local/bin/zeroclaw
ENTRYPOINT ["zeroclaw", "serve"]
```

2. Build and tag the image:
```bash
podman build -t zeroclaw:local .
```

3. Create a Podman quadlet at `~/.config/containers/systemd/zeroclaw.container`

4. Disable the current host service and remove the binary:
```bash
systemctl --user disable --now zeroclaw
rm ~/.local/bin/zeroclaw
```

5. Enable the quadlet service instead

This is a well-defined migration with no data loss â€” the memory database at `/mnt/hdd/projects/zeroclaw/memory.db` is bind-mounted and carries over unchanged.

---

**See also**: [DEPLOYMENT_GUIDE_STABLE_V3.md](DEPLOYMENT_GUIDE_STABLE_V3.md) | [OPERATIONS.md](OPERATIONS.md) | [TROUBLESHOOTING.md](TROUBLESHOOTING.md)

## Memory and Conversation Recall

ZeroClaw's memory backend is SQLite (`brain.db` in the workspace). It saves memories automatically but retrieval quality depends on the embedding provider.

### Default (no embeddings) â€” keyword search only

With `embedding_provider = "none"`, ZeroClaw can only do basic keyword matching. It will not proactively surface relevant memories â€” you need to give it keywords to search against. This is limited.

### Recommended â€” Ollama embeddings (free, local)

Pull the embedding model:
```bash
ollama pull nomic-embed-text
```

Update `~/.zeroclaw/config.toml`:
```toml
[memory]
backend = "sqlite"
auto_save = true
embedding_provider = "ollama"
embedding_model = "nomic-embed-text"
```

Restart:
```bash
systemctl --user restart zeroclaw
```

This enables semantic search â€” ZeroClaw surfaces relevant memories based on meaning rather than exact keywords. `nomic-embed-text` is ~270MB and runs quickly on the i5-8250U.

### Memory files location
```
~/.zeroclaw/workspace/memory/
â”œâ”€â”€ brain.db        # main memory database
â”œâ”€â”€ brain.db-shm    # shared memory file
â”œâ”€â”€ brain.db-wal    # write-ahead log
â””â”€â”€ archive/        # archived older memories
```

---

**Status**: v2.0 â€” Claude Haiku via LiteLLM as recommended default, correct Anthropic model names with anthropic/ prefix, anthropic-custom: pitfall documented, full provider status updated for v0.1.1  
**Last Updated**: February 2026
